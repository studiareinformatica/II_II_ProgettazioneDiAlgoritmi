Si definiscono \textit{algoritmi approssimanti} quelli che cercano soluzioni ammissibili: non si impongono di trovare le soluzioni ottimali e precise a problemi specifici, ma aggirano il problema elaborando soluzioni semplificate e quantomeno ammissibili.

\paragraph{Euristica}
Qualcosa che risolve problemi \textit{come meglio può}, per cui non è esattamente dimostrabile la correttezza.

\section{Vertex-cover}
Partiamo da un grafo a \textit{n} nodi: il problema del \textit{vertex-cover} vuole selezionare il numero minimo di nodi tale che tutti gli archi del grafo coprano almeno uno di questi nodi.
Si tratta di un problema di minimizzazione: ovviamente prendendo tutti i nodi, copriremo anche tutti gli archi che coprono due volte alcuni nodi. \\
La soluzione ottima, di conseguenza, costa $O(1)$, mentre la peggiore $O(n)$.
Ho un'euristica, se questo algoritmo non sbaglia, allora il \textit{rapporto d'approssimazione}:
\begin{center}
	$\frac{c(soluzione trovata)}{c(soluzione ottima)} \geq \frac{n}{1}$
\end{center}
sarà pari ad 1. Più l'algoritmo si allontana da una situazione ottimale, più sarà lontano (minore) da 1. \\
L'obiettivo è trovare un valore di euristica che sia sempre $\leq 2$ (si è dimostrato che sotto 2 non si possa andare).

\paragraph{NB}
Molto spesso algoritmi \textit{greedy} vanno incontro ai fini dell'euristica, sebbene non risultano essere sempre \textit{ottimi}. \\ \\

L'idea tramite algoritmo \textit{greedy} è quello di cercare il nodo che sia collegato al numero maggiore di archi, escluderlo, insieme con tutti gli archi a cui era legato. Ripetendo questa procedura fino a quando non avremo più archi, tutti i nodi esclusi fino a quel momento faranno parte dell'insieme \textit{vertex-cover}. \\
Occorre quindi ora provare la validità di questo algoritmo.
Prendendo una catena di 5 nodi, sappiamo che la soluzione ottima è di $O(2)$, prendendo il secondo e il quarto nodo. Il nostro algoritmo - se partirà dal terzo nodo -, si ritroverà a dover analizzare ancora almeno altri 2 nodi: avrà costo pari a $O(3)$. Per questo l'euristica avrà \textit{rapporto d'approssimazione} pari (circa) a \textit{1,5}, perché:
\begin{center}
	$\frac{c(soluzione trovata)}{c(soluzione ottima)} = \frac{O(3)}{O(2)} = 1,5$
\end{center}
Detto ciò, però, utilizzando altri esempi possiamo arrivare a soluzioni che abbiano \textit{rapporto d'approssimazione} decisamente maggiore di 2. \\
\begin{algorithm}
	\caption{Algoritmo approssimamente vero per vertex-cover}\label{alg:VC1}
	\begin{algorithmic}[1]
		\Function{algo}{}
		\State $Sol \gets \phi$  \Comment{Inizializzazione insieme nodi}
		\While {$ E \neq \phi $}
		\State estrai arco \textit{\{x,y\}} da \textit{E}
		\If { $x \notin Sol$ and $y \notin Sol$}
		\State $Sol \gets Sol \cup \{x,y\}$
		\EndIf
		\EndWhile
		\State \textbf{return} \textit{Sol}
		\EndFunction
	\end{algorithmic}
\end{algorithm} \hfill \\

Questo algoritmo ha complessità $O(m)$. Inoltre, è certamente ammissibile, perché: supponiamo che per assurdo non lo faccia; c'è un arco per cui non è stato preso né un estremo né l'altro. Ci sarà stato un momento, tramite il \textit{while} in cui è stato analizzato l'arco \textit{\{a,b\}}: ma proprio per come è stato scritto l'\textit{if}, sicuramente o in un altro momento era già stato preso uno dei due estremi, o altrimenti - se così non fosse stato - l'\textit{if} avrebbe portato a prendere entrambi. \\
D'altronde, l'algoritmo non è perfetto perché per ogni arco preso in considerazione, inserisce nell'insieme entrambi i nodi, quando non sarebbe sempre assolutamente necessario. \\
Proprio per la funzione dell'\textit{if}, ad ogni passaggio del \textit{while}, l'algoritmo includerà nell'insieme \textit{Sol} un numero \textit{K} di archi che legano nodi sicuramente differenti per ogni \textit{K}, che ammontano quindi a \textit{2K} (perché per ogni arco di \textit{K}, questo lega due nodi tra loro). Di conseguenza, sapendo che la soluzione ottimale porterebbe ad un numero di nodi pari ad \textit{almeno} \textit{K} (perché di fatto, l'algoritmo prende per ogni \textit{K} entrambi i nodi, ma non sappiamo se in realtà necessiti di uno solo dei due), il \textit{rapporto d'approssimazione} risultante è il seguente:
\begin{center}
	$\frac{c(soluzione trovata)}{c(soluzione ottima)} = \frac{2K}{almeno K} \leq \frac{2K}{K} = 2$
\end{center}

\section{Massimizzazione e minimizzazione}
Si suppone di avere un dato insieme di oggetti $\{P_{1}, P_{2}, \ldots, P_{n}\}$ ciascuno con dato peso, con un contenitore di capacità massima \textit{C}. Abbiamo due problemi simili (che si vedrà hanno però  complessità diversa):
\begin{enumerate}
	\item individuare il sottoinsieme di dimensione massima degli oggetti che entrino nel contenitore (problema di massimizzazione, risolvibile in tempo polinomiale);
	\item saturare il contenitore riempiendolo col minor numero possibile di oggetti (problema di minimizzazione).
\end{enumerate}

\subsection{Problema di massimizzazione}
Un semplicissimo algoritmo \textit{greedy} prenderebbe - in ordine crescente di dimensione - il primo oggetto e lo inserirebbe nel contenitore. Troverebbe poi il prossimo: qualora entrasse per dimensione nel contenitore, lo inserirebbe, altrimenti si fermerebbe. Ripeterebbe quindi questo passaggio finché ci sarebbe spazio a sufficienza. Questo algoritmo trova una soluzione ammissibile: non c'è un altro insieme che possa trovare una soluzione migliore la soluzione al problema: $O(n\times \log{n})$ per ordinarlo, e $O(n)$ per saturare il contenitore.

\subsection{Problema di minimizzazione}
Analogamente, un algoritmo potrebbe prendere ad ogni passo l'oggetto che occupa più spazio, specularmente all'algo precedente. Quando arriva al momento in cui non riesce ad entrare il prossimo oggetto, non si ferma, ma continua a verificare con oggetti sempre più piccoli, finché c'è spazio. \\ Stavolta la cosa non funziona come desiderato. Avendo infatti tre oggetti $\{7, 5, 5, 1, 1\}$ con contenitore di capienza $C=10$, prenderebbe il \textit{7}, senza poi poter prendere i \textit{5}; prenderebbe allora i due \textit{1}, arrivando a \textit{9}; la soluzione migliore però sarebbe quella di prendere soltanto i due \textit{5}, saturando completamente lo spazio, con un numero minore di elementi, mentre nella nostra soluzione abbiamo avuto bisogno di prendere \textit{3} elementi per poterlo fare. Infatti, il nostro obiettivo è unicamente trovare il numero minimo di oggetti per saturarlo, non quello di trovarne il minor numero per arrivare a saturare \textit{meglio} il contenitore. \\
Quindi l'euristica proposta non funziona: infatti, non esistono in assoluto soluzioni al problema note.
Una soluzione potrebbe essere confrontare tutti i possibili sottoinsiemi degli oggetti, tenendo quello di dimensione più piccola, il che porterebbe già a $\Omega(2^n)$.
\newpage

\section{Divide et Impera}
Ho un grosso problema: fosse più piccolo sarebbe più semplice. L'approccio \textit{divide et impera} divide in 2 (o più) sottoproblemi (divisibili, eventualmente, a loro volta, in altri sottoproblemi), analizzandoli e risolvendoli in maniera indipendente l'uno dall'altro, per poi ricomporre le varie sottosoluzioni:
\begin{center}
	$T(n) = a\times T(\frac{n}{b}) + O(f(n))$ \\
	dove \textit{T(n)} è il tempo totale, e \textit{a} e \textit{b} sono costanti generalmente settate a 2.
\end{center}
In un buon \textit{divide et impera}, bisogna cercare di tenere la scomposizione (il costo dei sottoproblemi) il più bilanciato possibile.

\subsection{Teorema Master}
Il \textit{master theorem} serve come ricettario per arrivare alla complessità di un algoritmo che sfrutta l'approccio \textit{divide et impera}, articolandosi in tre casistiche:
\begin{enumerate}
	\item $\log_{b}{a} > c \Rightarrow \Theta(n^{\log_{b}{a}})$
	\item $\log_{b}{a} = c \Rightarrow O(n^c\times \log^{k+1}{n})$
	\item $\log_{b}{a} < c \Rightarrow O(n^c\times \log^k{n})$
\end{enumerate}

\paragraph{Esercizio}
Ho un vettore con \textit{n} interi distinti: voglio scrivere una procedura $Sel(V,k)$ che selezioni il \textit{k}-esimo elemento del vettore - dopo averlo ordinato. \\
Il primo metodo è applicare un ordinamento sul vettore e poi selezionare - in $O(1)$ - l'elemento \textit{k}: questo metodo ha complessità $O(n\times \log{n})$ (dove proprio $n \times \log{n}$ è il costo minimo dell'ordinamento). \\
Possiamo poi pensare che \textit{k} possa essere o il primo o l'ultimo elemento, in questo caso occorrerebbe rispettivamente ricavarsi o il minimo o il massimo del vettore: in entrambi i casi, costerebbe $O(n)$. \\
Il primo metodo però non permette di sapere soltanto l'elemento - nel vettore ordinato equivalente - in posizione \textit{k}, ma potenzialmente un qualsiasi elemento in posizione da 0 a \textit{n}: sembra fare quindi molto di più rispetto a quanto necessario. \\
L'alternativa potrebbe invece essere l'applicazione di \textit{Dijkstra}, attraverso cui potremmo calcolarci il cammino minimo per raggiungere \textit{k}: questo - ugualmente - andrebbe a costarci tot, quando in realtà tutte le funzionalità che offre non sono assolutamente necessarie.
Questo soltanto per dire che il discorso di pensare che un algoritmo che fa più di quanto assolutamente necessario dalla consegna non necessariamente è più complesso. \\
Vogliamo scrivere una procedura che lavori in $O(n)$, che lavori in \textit{divide et impera} con una sorta di \textit{quickSort}. L'idea è quella di evitare di ordinare completamente il vettore, ma di farlo soltanto fino all'elemento \textit{k}: con \textit{quickSort} si prendeva un perno da cui partire randomicamente, nel nostro caso partiamo dal primo elemento del vettore in input.
\newpage
L'algoritmo ci suddividerà il vettore in due sub-vettori, dove il punto di separazione è proprio il perno:
\begin{itemize}
	\item insieme A, che contiene tutti gli elementi $< x$, dove $x$ è il perno;
	\item insieme B, che contiene tutti gli elementi $\geq x$.
\end{itemize}
A questo punto verifichiamo che la cardinalità di \textit{A} sia $\geq k$: se così fosse, applichiamo lo stesso algoritmo ricorsivamente sullo stesso perno, ma sul sub-vettore \textit{A} ($Sel(A,k)$); altrimenti, lo applichiamo sul perno $k-|A|$ e sub-vettore \textit{B} ($Sel(B, k-|A|$). Decidiamo allora di introdurre un nuovo input alla procedura \textit{n}, che indichi la capienza del vettore, che ci permetta di analizzare ad ogni passaggio la grandezza del vettore rimanente ricorsivamente (inserendo uno \textit{statement} all'inizio della procedura, che ci farà ritornare l'unico elemento del vettore, se $n=1$). \\
\begin{algorithm}
	\caption{Algoritmo individuazione elemento in posizione K del vettore ordinato V}\label{alg:KinV}
	\begin{algorithmic}[1]
		\Function{algo}{$V, k, n$}
			\If { $n=1$ }
				\State \Return $V[1]$
			\EndIf
			\State $x \gets V[1]$
			\State $contA = contB = \phi$
			\For { $i \gets 2$ to $n$ }
				\If { $V[i] < x$ }
					\State $contA++$
					\State $A[contA] \gets V[i]$
				\Else
					\State $contB++$
					\State $B[contB] \gets V[i]$
				\EndIf
			\EndFor
			\If { $contA = k-1$ }
				\State \Return $x$
			\EndIf
			\If { $contA \geq k $ }
				\State \Return $Sel(A, k, contA)$
			\Else
				\State \Return $Sel(B, k-(contA+1), contB)$
			\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm} \hfill \\
Analizziamo ora il costo di tempo della procedura:
\begin{itemize}
	\item il \textit{for} ci costa \textit{n}: $O(n)$;
	\item il restante dipende da \textit{A} e \textit{B}: $T(max\{contA,contB\})$ - nel caso pessimo.
\end{itemize}
Ora cerchiamo di lavorare sul tempo calcolato nel caso pessimo: il \textit{max} può variare da 1 a $n-1$. Quindi nel caso pessimo, $T(n) \leq T(n-1) + O(n)$. Nel caso più bilanciato possibile, invece: $T(n) \leq T(\frac{n}{2}) + O(n)$. La prima, utilizzando il \textit{master theorem}, ci porta a $O(n^2)$, mentre la seconda a $O(n)$. La verità quindi si suppone sia una via di mezzo tra questi ultimi. Nella maggior parte dei casi accade la seconda, probabilisticamente parlando, ma lo vogliamo dimostrare. \\
Per il \textit{master theorem}: $\log_{b}{a} = \log_{2}{1} = 0$. Ma abbiamo detto che in questo caso, il perno occorre che sia al centro, perfettamente bilanciato. Calcoliamo il caso in cui il perno sia - invece che ad $\frac{1}{2}$ - ad $\frac{1}{4}$: la ricorrenza sarà $T(n) \leq T(\frac{3n}{4}) +O(n)$. Quindi, $\log_{\frac{4}{3}}{1} = 0$. A maggior ragione, non è necessario che il perno sia perfettamente bilanciato, perché la ricorrenza sia $\leq T(\frac{n}{2}) + O(n)$. Ancora una volta, se il perno si trova a $\frac{1}{100}$, ancora il $\log_{\frac{100}{99}}{1} = 0$, e quindi la ricorrenza precedente sarà ancora valida. \\
Di conseguenza, è verificato che - fintanto che l'insieme maggiore è una frazione dell'insieme originale - la normalità sembra essere quella in cui la ricorrenza è rappresentata da:
\begin{center}
	$T(n) \leq T(\frac{n}{2}) + O(n)$ \\
	e quindi, la complessità: $O(n)$
\end{center}
Per garantire che questo sia il caso della nostra procedura, occorre che - una volta scelto un elemento randomicamente nel vettore - questo sia scambiato con il primo elemento del vettore. Occorre quindi aggiungere questo snippet in testa alle istruzioni della procedura:
\begin{algorithm}
	\label{alg:KinVPatch}
	\begin{algorithmic}[1]
		\State $r \gets Random(1,n)$
		\State scambia $V[1]$ con $V[r]$
		\State $x \gets V[1]$
	\end{algorithmic}
\end{algorithm} \hfill \\

\paragraph{Alternativa}
Ma questo è un algoritmo che garantisce $O(n)$ nel caso medio probabilistico. Ne esiste anche uno che garantisce $O(n)$ nel caso peggiore probabilistico: prendiamo una matrice $A \times B = C$, nata dal prodotto di due matrici \textit{A} ($n \times n$) e \textit{B} ($n \times n$). Per molto tempo si è pensato che il costo minimo di una procedura che eseguisse questo prodotto fosse di $n^3$ (dove \textit{n} è l'ampiezza delle matrici, ovviamente quadrate). Col tempo si è passati a considerarlo invece $n^{2.7}$, poi ancora $n^{2.3}$, utilizzando il metodo \textit{divide et impera} (con \textit{lower bound $\Omega(n^2)$}).
\newpage

\paragraph{Problema}
Ho \textit{n} punti in un piano: mi interessa la coppia di punti più vicini. \\
Un primo algoritmo banale sarebbe confrontare tutte le coppie: le combinazioni saranno pari a $(n$su$2) \approx O(n^2)$. A questo va aggiunto il tempo per calcolare la distanza fra due punti: $\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$. \\
Il problema è che non è possibile calcolare la radice in $O(1)$: utilizziamo quindi il calcolo del quadrato della distanza, così da poterla calcolare in $O(1)$, raggiungendo come complessità globale $O(n^2)$. \\
Supponiamo di lavorare su una sequenza di punti che hanno soltanto una coordinata dell'ascissa \textit{x}, perché si trovano tutti sulla stessa ordinata: per capire la coppia di punti più vicini, occorre solamente ordinarli ($O(n \times \log{n})$) e poi scorrerli per leggere le distanze di ciascuno ($O(n)$). Costerà $O(n \times \log{n})$. \\
Tornando al primo problema, l'idea per ottimizzare al massimo è quella di utilizzare il metodo \textit{divide et impera}, dividendo quindi in due sottoproblemi:
\begin{center}
	$T(n) = 2T(\frac{n}{2}) + O($ composizione scomposizione $) + O($ costo composizione $)$ \\
	dove il costo di composizione/scomposizione deve raggiungere - sommato - al massimo $O(n)$.
\end{center}
Supponiamo di partizionare l'insieme di punti in due sottoinsiemi, inserendo un asse delle ordinate - per esempio - nel gruppo di punti: l'importante è che la scomposizione venga fatta sulla componente \textit{x} di ciascun punto, ordinando poi - ancora sulla componente \textit{x} - in $O(n \times \log{n})$. Usiamo poi l'algoritmo \textit{divide et impera} dividendo il problema in due sottoproblemi bilanciati. \\
Supponiamo che ho ottnuto la coppia più vicina nei 2 sottoproblemi. Quanto tempo ci metto a ricomporli? Non possiamo scegliere il minore tra i due $\delta$ (dove $\delta_1$ è la coppia meno distante nel sottoinsieme \textit{A} e $\delta_2$ è la coppia meno distante nel sottoinsieme \textit{B}), perché ci potrebbe essere una distanza ancora minima tra un punto di \textit{A} ed un punto di \textit{B}. Occorre quindi trovare un metodo valido per comporre i due sottoproblemi adeguatamente.