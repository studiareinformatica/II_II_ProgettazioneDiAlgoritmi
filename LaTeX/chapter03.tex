Si definiscono \textit{algoritmi approssimanti} quelli che cercano soluzioni ammissibili: non si impongono di trovare le soluzioni ottimali e precise a problemi specifici, ma aggirano il problema elaborando soluzioni semplificate e quantomeno ammissibili.

\paragraph{Euristica}
Qualcosa che risolve problemi \textit{come meglio può}, per cui non è esattamente dimostrabile la correttezza.

\section{Vertex-cover}
Partiamo da un grafo a \textit{n} nodi: il problema del \textit{vertex-cover} vuole selezionare il numero minimo di nodi tale che tutti gli archi del grafo coprano almeno uno di questi nodi.
Si tratta di un problema di minimizzazione: ovviamente prendendo tutti i nodi, copriremo anche tutti gli archi che coprono due volte alcuni nodi. \\
La soluzione ottima, di conseguenza, costa $O(1)$, mentre la peggiore $O(n)$.
Ho un'euristica, se questo algoritmo non sbaglia, allora il \textit{rapporto d'approssimazione}:
\begin{center}
	$\frac{c(soluzione trovata)}{c(soluzione ottima)} \geq \frac{n}{1}$
\end{center}
sarà pari ad 1. Più l'algoritmo si allontana da una situazione ottimale, più sarà lontano (minore) da 1. \\
L'obiettivo è trovare un valore di euristica che sia sempre $\leq 2$ (si è dimostrato che sotto 2 non si possa andare).

\paragraph{NB}
Molto spesso algoritmi \textit{greedy} vanno incontro ai fini dell'euristica, sebbene non risultano essere sempre \textit{ottimi}. \\ \\

L'idea tramite algoritmo \textit{greedy} è quello di cercare il nodo che sia collegato al numero maggiore di archi, escluderlo, insieme con tutti gli archi a cui era legato. Ripetendo questa procedura fino a quando non avremo più archi, tutti i nodi esclusi fino a quel momento faranno parte dell'insieme \textit{vertex-cover}. \\
Occorre quindi ora provare la validità di questo algoritmo.
Prendendo una catena di 5 nodi, sappiamo che la soluzione ottima è di $O(2)$, prendendo il secondo e il quarto nodo. Il nostro algoritmo - se partirà dal terzo nodo -, si ritroverà a dover analizzare ancora almeno altri 2 nodi: avrà costo pari a $O(3)$. Per questo l'euristica avrà \textit{rapporto d'approssimazione} pari (circa) a \textit{1,5}, perché:
\begin{center}
	$\frac{c(soluzione trovata)}{c(soluzione ottima)} = \frac{O(3)}{O(2)} = 1,5$
\end{center}
Detto ciò, però, utilizzando altri esempi possiamo arrivare a soluzioni che abbiano \textit{rapporto d'approssimazione} decisamente maggiore di 2. \\
\begin{algorithm}
	\caption{Algoritmo approssimamente vero per vertex-cover}\label{alg:VC1}
	\begin{algorithmic}[1]
		\Function{algo}{}
		\State $Sol \gets \phi$  \Comment{Inizializzazione insieme nodi}
		\While {$ E \neq \phi $}
		\State estrai arco \textit{\{x,y\}} da \textit{E}
		\If { $x \notin Sol$ and $y \notin Sol$}
		\State $Sol \gets Sol \cup \{x,y\}$
		\EndIf
		\EndWhile
		\State \textbf{return} \textit{Sol}
		\EndFunction
	\end{algorithmic}
\end{algorithm} \hfill \\

Questo algoritmo ha complessità $O(m)$. Inoltre, è certamente ammissibile, perché: supponiamo che per assurdo non lo faccia; c'è un arco per cui non è stato preso né un estremo né l'altro. Ci sarà stato un momento, tramite il \textit{while} in cui è stato analizzato l'arco \textit{\{a,b\}}: ma proprio per come è stato scritto l'\textit{if}, sicuramente o in un altro momento era già stato preso uno dei due estremi, o altrimenti - se così non fosse stato - l'\textit{if} avrebbe portato a prendere entrambi. \\
D'altronde, l'algoritmo non è perfetto perché per ogni arco preso in considerazione, inserisce nell'insieme entrambi i nodi, quando non sarebbe sempre assolutamente necessario. \\
Proprio per la funzione dell'\textit{if}, ad ogni passaggio del \textit{while}, l'algoritmo includerà nell'insieme \textit{Sol} un numero \textit{K} di archi che legano nodi sicuramente differenti per ogni \textit{K}, che ammontano quindi a \textit{2K} (perché per ogni arco di \textit{K}, questo lega due nodi tra loro). Di conseguenza, sapendo che la soluzione ottimale porterebbe ad un numero di nodi pari ad \textit{almeno} \textit{K} (perché di fatto, l'algoritmo prende per ogni \textit{K} entrambi i nodi, ma non sappiamo se in realtà necessiti di uno solo dei due), il \textit{rapporto d'approssimazione} risultante è il seguente:
\begin{center}
	$\frac{c(soluzione trovata)}{c(soluzione ottima)} = \frac{2K}{almeno K} \leq \frac{2K}{K} = 2$
\end{center}

\section{Massimizzazione e minimizzazione}
Si suppone di avere un dato insieme di oggetti $\{P_{1}, P_{2}, \ldots, P_{n}\}$ ciascuno con dato peso, con un contenitore di capacità massima \textit{C}. Abbiamo due problemi simili (che si vedrà hanno però  complessità diversa):
\begin{enumerate}
	\item individuare il sottoinsieme di dimensione massima degli oggetti che entrino nel contenitore (problema di massimizzazione, risolvibile in tempo polinomiale);
	\item saturare il contenitore riempiendolo col minor numero possibile di oggetti (problema di minimizzazione).
\end{enumerate}

\subsection{Problema di massimizzazione}
Un semplicissimo algoritmo \textit{greedy} prenderebbe - in ordine crescente di dimensione - il primo oggetto e lo inserirebbe nel contenitore. Troverebbe poi il prossimo: qualora entrasse per dimensione nel contenitore, lo inserirebbe, altrimenti si fermerebbe. Ripeterebbe quindi questo passaggio finché ci sarebbe spazio a sufficienza. Questo algoritmo trova una soluzione ammissibile: non c'è un altro insieme che possa trovare una soluzione migliore la soluzione al problema: $O(n\times \log{n})$ per ordinarlo, e $O(n)$ per saturare il contenitore.

\subsection{Problema di minimizzazione}
Analogamente, un algoritmo potrebbe prendere ad ogni passo l'oggetto che occupa più spazio, specularmente all'algo precedente. Quando arriva al momento in cui non riesce ad entrare il prossimo oggetto, non si ferma, ma continua a verificare con oggetti sempre più piccoli, finché c'è spazio. \\ Stavolta la cosa non funziona come desiderato. Avendo infatti tre oggetti $\{7, 5, 5, 1, 1\}$ con contenitore di capienza $C=10$, prenderebbe il \textit{7}, senza poi poter prendere i \textit{5}; prenderebbe allora i due \textit{1}, arrivando a \textit{9}; la soluzione migliore però sarebbe quella di prendere soltanto i due \textit{5}, saturando completamente lo spazio, con un numero minore di elementi, mentre nella nostra soluzione abbiamo avuto bisogno di prendere \textit{3} elementi per poterlo fare. Infatti, il nostro obiettivo è unicamente trovare il numero minimo di oggetti per saturarlo, non quello di trovarne il minor numero per arrivare a saturare \textit{meglio} il contenitore. \\
Quindi l'euristica proposta non funziona: infatti, non esistono in assoluto soluzioni al problema note.
Una soluzione potrebbe essere confrontare tutti i possibili sottoinsiemi degli oggetti, tenendo quello di dimensione più piccola, il che porterebbe già a $\Omega(2^n)$.
\newpage

\section{Divide et Impera}
Ho un grosso problema: fosse più piccolo sarebbe più semplice. L'approccio \textit{divide et impera} divide in 2 (o più) sottoproblemi (divisibili, eventualmente, a loro volta, in altri sottoproblemi), analizzandoli e risolvendoli in maniera indipendente l'uno dall'altro, per poi ricomporre le varie sottosoluzioni:
\begin{center}
	$T(n) = a\times T(\frac{n}{b}) + O(f(n))$ \\
	dove \textit{T(n)} è il tempo totale, e \textit{a} e \textit{b} sono costanti generalmente settate a 2.
\end{center}
In un buon \textit{divide et impera}, bisogna cercare di tenere la scomposizione (il costo dei sottoproblemi) il più bilanciato possibile.

\subsection{Teorema Master}
Il \textit{master theorem} serve come ricettario per arrivare alla complessità di un algoritmo che sfrutta l'approccio \textit{divide et impera}, articolandosi in tre casistiche:
\begin{enumerate}
	\item $\log_{b}{a} > c \Rightarrow \Theta(n^{\log_{b}{a}})$
	\item $\log_{b}{a} = c \Rightarrow O(n^c\times \log^{k+1}{n})$
	\item $\log_{b}{a} < c \Rightarrow O(n^c\times \log^k{n})$
\end{enumerate}